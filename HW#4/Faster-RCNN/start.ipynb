{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6ee056",
   "metadata": {},
   "source": [
    "# Faster R-CNN\n",
    "### 데이터 : coco128\n",
    "- faster r-cnn에 맞는 데이터는 아니지만 다른 모델과 비교하기위해서 임의로 사용\n",
    "- 모델특징 : 작은 객체 탐지에 강력하나, 학습에 자원이 많이들고 까다로움.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fae147",
   "metadata": {},
   "source": [
    "1. coco 128기반으로 데이터 어노테이션 json 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c7399dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 764.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved COCO JSON to: datasets\\annotations\\instances_train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 605.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved COCO JSON to: datasets\\annotations\\instances_val.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "def convert_yolo_to_coco(img_dir, label_dir, output_json, categories):\n",
    "    coco = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "\n",
    "    ann_id = 1\n",
    "    img_id = 1\n",
    "\n",
    "    for cat_id, cat_name in enumerate(categories, 1):\n",
    "        coco[\"categories\"].append({\"id\": cat_id, \"name\": cat_name})\n",
    "\n",
    "    for img_file in tqdm(sorted(os.listdir(img_dir))):\n",
    "        if not img_file.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        label_path = os.path.join(label_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        coco[\"images\"].append({\n",
    "            \"file_name\": img_file,\n",
    "            \"height\": h,\n",
    "            \"width\": w,\n",
    "            \"id\": img_id\n",
    "        })\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                parts = list(map(float, line.strip().split()))\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                class_id, x_center, y_center, width, height = parts\n",
    "\n",
    "                x = (x_center - width / 2) * w\n",
    "                y = (y_center - height / 2) * h\n",
    "                bw = width * w\n",
    "                bh = height * h\n",
    "\n",
    "                coco[\"annotations\"].append({\n",
    "                    \"id\": ann_id,\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": int(class_id) + 1,\n",
    "                    \"bbox\": [x, y, bw, bh],\n",
    "                    \"area\": bw * bh,\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "                ann_id += 1\n",
    "\n",
    "        img_id += 1\n",
    "\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(coco, f, indent=4)\n",
    "\n",
    "    print(f\"Saved COCO JSON to: {output_json}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    categories = [\n",
    "        \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\",\n",
    "        \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "        \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\",\n",
    "        \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
    "        \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\n",
    "        \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\",\n",
    "        \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "        \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\",\n",
    "        \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\",\n",
    "        \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\",\n",
    "        \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "    ]    \n",
    "    root_dir = \"datasets\"\n",
    "\n",
    "\n",
    "    convert_yolo_to_coco(\n",
    "        img_dir=os.path.join(root_dir, \"images\",\"train\"),\n",
    "        label_dir=os.path.join(root_dir, \"labels\",\"train\"),\n",
    "        output_json=os.path.join(root_dir, \"annotations\",\"instances_train.json\"),\n",
    "        categories=categories\n",
    "    )\n",
    "\n",
    "    convert_yolo_to_coco(\n",
    "        img_dir=os.path.join(root_dir, \"images\",\"val\"),\n",
    "        label_dir=os.path.join(root_dir, \"labels\",\"val\"),\n",
    "        output_json=os.path.join(root_dir, \"annotations\",\"instances_val.json\"),\n",
    "        categories=categories\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f8feb",
   "metadata": {},
   "source": [
    "2. 사전학습 모델을 가져와 카테고리(클래스) 값만 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b92312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\datascn\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\datascn\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "num_classes = len(categories) + 1  # background 포함\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4fabae",
   "metadata": {},
   "source": [
    "3. faster RCNN 모델학습 데이터 가공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# COCO annotation 형식의 사용자 정의 Dataset 클래스\n",
    "class CocoLikeDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_file, transforms=None):\n",
    "        self.img_dir = img_dir  # 이미지 파일들이 들어있는 디렉토리\n",
    "        self.transforms = transforms  # 증강포함\n",
    "\n",
    "        # annotation JSON 파일 로드\n",
    "        with open(ann_file, 'r') as f:\n",
    "            self.coco = json.load(f)\n",
    "\n",
    "        # class id- 이름을 매핑 \n",
    "        self.categories = {cat['id']: cat['name'] for cat in self.coco['categories']}\n",
    "\n",
    "        # id별 어노테이션 그룹핑\n",
    "        self.image_id_to_annotations = {}\n",
    "        for ann in self.coco['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.image_id_to_annotations:\n",
    "                self.image_id_to_annotations[img_id] = []\n",
    "            self.image_id_to_annotations[img_id].append(ann)\n",
    "\n",
    "        # image 정보리스트\n",
    "        self.images = self.coco['images']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 현재 인덱스에 해당하는 이미지 정보\n",
    "        img_info = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])  # 이미지 경로 \n",
    "        img = Image.open(img_path).convert(\"RGB\")  #이미지 불러오기\n",
    "\n",
    "        image_id = img_info['id']\n",
    "        anns = self.image_id_to_annotations.get(image_id, [])  # 해당이미지 어노테이션목록\n",
    "\n",
    "        boxes = []     # 바운딩 박스 좌표 [x_min, y_min, x_max, y_max]\n",
    "        labels = []    # 클래스 ID\n",
    "        areas = []     # 객체 면적\n",
    "        iscrowd = []   # crowd 여부 (0: 단일 객체, 1: 군집 객체)\n",
    "\n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']  # COCO 형식: [x, y, width, height]\n",
    "            x_min = bbox[0]\n",
    "            y_min = bbox[1]\n",
    "            x_max = x_min + bbox[2]\n",
    "            y_max = y_min + bbox[3]\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(ann['category_id'])        # 분류 ID\n",
    "            areas.append(ann['area'])                # 사전 계산된 면적\n",
    "            iscrowd.append(ann.get('iscrowd', 0))    # 'iscrowd'가 없으면 0으로 설정\n",
    "\n",
    "        # 모든 리스트를 tensor로 변환 (PyTorch 모델 입력용)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        # Faster R-CNN 등에 맞는 포맷으로 타깃 딕셔너리 구성\n",
    "        target = {\n",
    "            \"boxes\": boxes,                   # [N, 4]\n",
    "            \"labels\": labels,                 # [N]\n",
    "            \"image_id\": torch.tensor([image_id]),  # [1]\n",
    "            \"area\": areas,                    # [N]\n",
    "            \"iscrowd\": iscrowd                # [N]\n",
    "        }\n",
    "\n",
    "        # 이미지 변환 적용 (ToTensor, Augmentation 등)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target  # 모델에 전달될 (이미지, 타깃)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)  # 전체 이미지 개수 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b1f649",
   "metadata": {},
   "source": [
    "4. 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ca85530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 41,481,636\n",
      "Epoch 1, Loss: 0.8909\n",
      "Epoch 2, Loss: 0.8434\n",
      "Epoch 3, Loss: 0.8598\n",
      "Epoch 4, Loss: 0.8807\n",
      "Epoch 5, Loss: 0.8425\n",
      "Epoch 6, Loss: 0.8520\n",
      "Epoch 7, Loss: 0.7724\n",
      "Epoch 8, Loss: 0.7576\n",
      "Epoch 9, Loss: 0.7457\n",
      "Epoch 10, Loss: 0.6941\n",
      "Epoch 11, Loss: 0.6681\n",
      "Epoch 12, Loss: 0.6897\n",
      "Epoch 13, Loss: 0.6568\n",
      "Epoch 14, Loss: 0.6153\n",
      "Epoch 15, Loss: 0.6215\n",
      "Epoch 16, Loss: 0.6244\n",
      "Epoch 17, Loss: 0.5546\n",
      "Epoch 18, Loss: 0.5749\n",
      "Epoch 19, Loss: 0.5390\n",
      "Epoch 20, Loss: 0.5056\n",
      "Epoch 21, Loss: 0.5425\n",
      "Epoch 22, Loss: 0.5339\n",
      "Epoch 23, Loss: 0.5183\n",
      "Epoch 24, Loss: 0.4876\n",
      "Epoch 25, Loss: 0.4741\n",
      "Epoch 26, Loss: 0.4683\n",
      "Epoch 27, Loss: 0.4667\n",
      "Epoch 28, Loss: 0.4930\n",
      "Epoch 29, Loss: 0.4273\n",
      "Epoch 30, Loss: 0.4087\n",
      "Epoch 31, Loss: 0.4127\n",
      "Epoch 32, Loss: 0.4239\n",
      "Epoch 33, Loss: 0.3789\n",
      "Epoch 34, Loss: 0.4109\n",
      "Epoch 35, Loss: 0.3917\n",
      "Epoch 36, Loss: 0.3542\n",
      "Epoch 37, Loss: 0.3729\n",
      "Epoch 38, Loss: 0.4004\n",
      "Epoch 39, Loss: 0.3645\n",
      "Epoch 40, Loss: 0.3513\n",
      "Epoch 41, Loss: 0.3438\n",
      "Epoch 42, Loss: 0.3243\n",
      "Epoch 43, Loss: 0.3338\n",
      "Epoch 44, Loss: 0.3493\n",
      "Epoch 45, Loss: 0.3023\n",
      "Epoch 46, Loss: 0.3092\n",
      "Epoch 47, Loss: 0.3064\n",
      "Epoch 48, Loss: 0.2964\n",
      "Epoch 49, Loss: 0.3224\n",
      "Epoch 50, Loss: 0.3032\n",
      "\n",
      "Total Training Time: 2459.12 seconds (40.99 minutes)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import time\n",
    "\n",
    "train_dataset = CocoLikeDataset(\n",
    "    img_dir=\"datasets/images/train\",\n",
    "    ann_file=\"datasets/annotations/instances_train.json\",\n",
    "    transforms=get_transform(train=True)\n",
    ")\n",
    "val_dataset = CocoLikeDataset(\n",
    "    img_dir=\"datasets/images/val\",\n",
    "    ann_file=\"datasets/annotations/instances_val.json\",\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 모델 매개변수 수 출력\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total Trainable Parameters: {num_params:,}\")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "num_epochs = 50\n",
    "start_time = time.time()  # 시작 시간\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k,v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "end_time = time.time()  # 끝 시간\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal Training Time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
